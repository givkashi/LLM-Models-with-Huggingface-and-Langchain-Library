{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [],
      "dockerImageVersionId": 30699,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Exploring LLM Models with Hugging Face and Langchain Library : A Comprehensive Guide**"
      ],
      "metadata": {
        "id": "-6G7WUkowoi3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Exploring different LLM models from Hugging Face with LangChain**\n",
        "\n",
        "\n",
        "**Llama, Mistral, Phi**"
      ],
      "metadata": {
        "id": "LsSDyALlwojA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 1: Setting Up the Environment**"
      ],
      "metadata": {
        "id": "w1cFMGH-wojD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U langchain transformers bitsandbytes accelerate"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "execution": {
          "iopub.status.busy": "2024-05-04T14:21:37.761657Z",
          "iopub.execute_input": "2024-05-04T14:21:37.762388Z",
          "iopub.status.idle": "2024-05-04T14:22:15.915757Z",
          "shell.execute_reply.started": "2024-05-04T14:21:37.762353Z",
          "shell.execute_reply": "2024-05-04T14:22:15.914739Z"
        },
        "trusted": true,
        "id": "gcHuuzi7wojF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os\n",
        "from langchain import PromptTemplate, HuggingFacePipeline\n",
        "from transformers import BitsAndBytesConfig, AutoModelForCausalLM, AutoTokenizer, GenerationConfig, pipeline\n",
        "from langchain_core.prompts import (\n",
        "    ChatPromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        "    MessagesPlaceholder,\n",
        ")\n",
        "from langchain_core.messages import SystemMessage"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-04T14:22:15.917596Z",
          "iopub.execute_input": "2024-05-04T14:22:15.917914Z",
          "iopub.status.idle": "2024-05-04T14:22:33.581187Z",
          "shell.execute_reply.started": "2024-05-04T14:22:15.917885Z",
          "shell.execute_reply": "2024-05-04T14:22:33.580237Z"
        },
        "trusted": true,
        "id": "NjGrJY7OwojJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"HF_TOKEN\"]='your_huggingface_API_key'"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-04T14:22:33.582322Z",
          "iopub.execute_input": "2024-05-04T14:22:33.582825Z",
          "iopub.status.idle": "2024-05-04T14:22:33.587227Z",
          "shell.execute_reply.started": "2024-05-04T14:22:33.582799Z",
          "shell.execute_reply": "2024-05-04T14:22:33.586311Z"
        },
        "trusted": true,
        "id": "NkaIua3jwojL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 2: Initializing the Language Model**"
      ],
      "metadata": {
        "id": "ZGs23PtYwojN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_NAME = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
        "# MODEL_NAME =\"mistralai/Mistral-7B-Instruct-v0.2\"\n",
        "# MODEL_NAME =\"meta-llama/Meta-Llama-3-8B\"\n",
        "# MODEL_NAME =\"microsoft/Phi-3-mini-4k-instruct\"\n",
        "# MODEL_NAME =\"microsoft/phi-1_5\"\n",
        "\n",
        "# Quantization is a technique used to reduce the memory and computation requirements\n",
        "# of deep learning models, typically by using fewer bits, 4 bits\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "# Initialization of a tokenizer for the language model,\n",
        "# necessary to preprocess text data for input\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Initialization of the pre-trained language model\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME, torch_dtype=torch.float16,\n",
        "    trust_remote_code=True,\n",
        "    device_map=\"auto\",\n",
        "    quantization_config=quantization_config\n",
        ")\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-04T14:22:33.589969Z",
          "iopub.execute_input": "2024-05-04T14:22:33.590393Z",
          "iopub.status.idle": "2024-05-04T14:24:33.086749Z",
          "shell.execute_reply.started": "2024-05-04T14:22:33.590357Z",
          "shell.execute_reply": "2024-05-04T14:24:33.085915Z"
        },
        "trusted": true,
        "id": "4V3b3UJiwojP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 3: Configuring Generation Settings**"
      ],
      "metadata": {
        "id": "NUKDKjgmwojS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuration of some generation-related settings\n",
        "generation_config = GenerationConfig.from_pretrained(MODEL_NAME)\n",
        "generation_config.max_new_tokens = 1024 # maximum number of new tokens that can be generated by the model\n",
        "generation_config.temperature = 0.7 # randomness of the generated tex\n",
        "generation_config.top_p = 0 # diversity of the generated text\n",
        "generation_config.do_sample = True # sampling during the generation process\n",
        "# generation_config.repetition_penalty = 1.15 # the degree to which the model should avoid repeating tokens in the generated text"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-04T14:24:33.088096Z",
          "iopub.execute_input": "2024-05-04T14:24:33.088765Z",
          "iopub.status.idle": "2024-05-04T14:24:33.125644Z",
          "shell.execute_reply.started": "2024-05-04T14:24:33.088722Z",
          "shell.execute_reply": "2024-05-04T14:24:33.124782Z"
        },
        "trusted": true,
        "id": "MaFRoGv_wojV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 4: Creating the Pipeline**"
      ],
      "metadata": {
        "id": "ir_H2bm7wojX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# A pipeline is an object that works as an API for calling the model\n",
        "# The pipeline is made of (1) the tokenizer instance, the model instance, and\n",
        "# some post-procesing settings. Here, it's configured to return full-text outputs\n",
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    return_full_text=True,\n",
        "    generation_config=generation_config,\n",
        ")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-04T14:24:33.126646Z",
          "iopub.execute_input": "2024-05-04T14:24:33.126933Z",
          "iopub.status.idle": "2024-05-04T14:24:33.132614Z",
          "shell.execute_reply.started": "2024-05-04T14:24:33.126905Z",
          "shell.execute_reply": "2024-05-04T14:24:33.131514Z"
        },
        "trusted": true,
        "id": "9Xh7bM-XwojY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# HuggingFace pipeline\n",
        "llm = HuggingFacePipeline(pipeline=pipe)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-04T14:24:33.133707Z",
          "iopub.execute_input": "2024-05-04T14:24:33.133989Z",
          "iopub.status.idle": "2024-05-04T14:24:33.143574Z",
          "shell.execute_reply.started": "2024-05-04T14:24:33.133966Z",
          "shell.execute_reply": "2024-05-04T14:24:33.142668Z"
        },
        "trusted": true,
        "id": "HwOBk6yLwojZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 5: Testing the Model**"
      ],
      "metadata": {
        "id": "xWw0vngCwoja"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = \"Write me a poem about Machine Learning.\""
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-04T14:24:33.14469Z",
          "iopub.execute_input": "2024-05-04T14:24:33.145003Z",
          "iopub.status.idle": "2024-05-04T14:24:33.156665Z",
          "shell.execute_reply.started": "2024-05-04T14:24:33.144981Z",
          "shell.execute_reply": "2024-05-04T14:24:33.155795Z"
        },
        "trusted": true,
        "id": "ld53VY-hwoja"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = llm.invoke(input_text)\n",
        "\n",
        "print(output)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-04T14:24:33.157901Z",
          "iopub.execute_input": "2024-05-04T14:24:33.158296Z",
          "iopub.status.idle": "2024-05-04T14:24:50.394242Z",
          "shell.execute_reply.started": "2024-05-04T14:24:33.158266Z",
          "shell.execute_reply": "2024-05-04T14:24:50.393289Z"
        },
        "trusted": true,
        "id": "PwmKXAlXwojb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 6: Further Testing with PromptTemplate and Chain**"
      ],
      "metadata": {
        "id": "brugYsSZwojc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "template = \"\"\"\n",
        "     Write me a poem about {topic}.\n",
        "\"\"\""
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-04T14:24:50.395568Z",
          "iopub.execute_input": "2024-05-04T14:24:50.396029Z",
          "iopub.status.idle": "2024-05-04T14:24:50.400771Z",
          "shell.execute_reply.started": "2024-05-04T14:24:50.395995Z",
          "shell.execute_reply": "2024-05-04T14:24:50.399904Z"
        },
        "trusted": true,
        "id": "_iu-LwFcwojc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topic = \"Machine Learning\""
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-04T14:24:50.403519Z",
          "iopub.execute_input": "2024-05-04T14:24:50.403788Z",
          "iopub.status.idle": "2024-05-04T14:24:50.412424Z",
          "shell.execute_reply.started": "2024-05-04T14:24:50.403765Z",
          "shell.execute_reply": "2024-05-04T14:24:50.411654Z"
        },
        "trusted": true,
        "id": "BU38z78gwojd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = PromptTemplate(input_variables=[\"topic\"], template=template)\n",
        "# Construct a Langchain Chain to connect the prompt template with the LLM\n",
        "chain = prompt | llm\n",
        "output = chain.invoke({\"topic\": topic})\n",
        "\n",
        "print(output)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-04T14:24:50.413511Z",
          "iopub.execute_input": "2024-05-04T14:24:50.413842Z",
          "iopub.status.idle": "2024-05-04T14:24:59.272738Z",
          "shell.execute_reply.started": "2024-05-04T14:24:50.413814Z",
          "shell.execute_reply": "2024-05-04T14:24:59.271774Z"
        },
        "trusted": true,
        "id": "JM7ODksZwoje"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 6: Further Testing with ChatPromptTemplate**"
      ],
      "metadata": {
        "id": "xo7eFAijwoje"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "topic = \"Machine Learning\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        SystemMessage(\n",
        "            content=(\n",
        "                  \"\"\" Write a poem related to the input topic in one paragraph\"\"\"\n",
        "            )\n",
        "        ),\n",
        "        HumanMessagePromptTemplate.from_template(\"```{topic}```\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "chain = prompt | llm\n",
        "output = chain.invoke({\"topic\": topic})\n",
        "\n",
        "print(output)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-04T14:24:59.274091Z",
          "iopub.execute_input": "2024-05-04T14:24:59.274367Z",
          "iopub.status.idle": "2024-05-04T14:25:11.46704Z",
          "shell.execute_reply.started": "2024-05-04T14:24:59.274344Z",
          "shell.execute_reply": "2024-05-04T14:25:11.466079Z"
        },
        "trusted": true,
        "id": "cknryK8Swojf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}